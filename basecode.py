# -*- coding: utf-8 -*-
"""QLHEFT_cleaned.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ruhVB1LFHZgFBaNEcCfnIW3u9DFs8TQ3

# QL-HEFT and HEFT Algorithm Experiments

This notebook allows you to compare different implementations (HEFT, QL-HEFT, QL-HEFT with different Q-table states, epsilon-greedy, etc.) for the same workflow scheduling problem.
"""

import matplotlib.pyplot as plt
import networkx as nx
import random
from collections import defaultdict
import numpy as np
import pandas as pd
import seaborn as sns
import math

E = [('T1', 'T2'), ('T1', 'T5'), ('T5', 'T3'),
     ('T2', 'T4'), ('T5', 'T6'), ('T5', 'T7'),
     ('T5', 'T8'), ('T3', 'T9'), ('T4', 'T9'),
     ('T6', 'T9'), ('T7', 'T9'), ('T8', 'T9')]
graph = nx.DiGraph(E)

# Computation costs: W[i][j] = cost of task i on processor j
W = [[10, 9, 8], [12, 10, 9], [21, 18, 16],
     [23, 20, 18], [39, 34, 29], [25, 21, 19],
     [16, 14, 13], [33, 29, 25], [19, 16, 14]]

# Communication costs: c[i][2] = cost from c[i][0] to c[i][1] if on different processors
c = [["T1", "T2", 7], ["T1", "T5", 8], ["T5", "T3", 4],
     ["T2", "T4", 5], ["T5", "T6", 9], ["T5", "T7", 8],
     ["T5", "T8", 9], ["T3", "T9", 9], ["T4", "T9", 10],
     ["T6", "T9", 9], ["T7", "T9", 10], ["T8", "T9", 8]]
task_list = [f"T{i}" for i in range(1,10)]
task_index = {t:i for i,t in enumerate(task_list)}
comm_cost = { (src, dst): cost for src, dst, cost in c }

def viable_tasks(scheduled):
    return [t for t in graph.nodes if t not in scheduled and set(graph.predecessors(t)).issubset(scheduled)]

"""## DAG Visulization"""

for layer, nodes in enumerate(nx.topological_generations(graph)):
    # `multipartite_layout` expects the layer as a node attribute, so add the
    # numeric layer value as a node attribute
    for node in nodes:
        graph.nodes[node]["layer"] = layer

# Compute the multipartite_layout using the "layer" node attribute
pos = nx.multipartite_layout(graph, subset_key="layer", align="horizontal")

fig, ax = plt.subplots()
nx.draw_networkx(graph, pos=pos, ax=ax)
ax.set_title("DAG layout in topological order")
fig.tight_layout()
plt.show()

# --- 2. Visualization Helper ---
def visualize_gantt_chart(task_schedule, processor_schedules, makespan, num_processors, title="Gantt Chart"):
    fig, ax = plt.subplots(figsize=(6, 4))
    colors = ['#90C9E7', '#219EBC', '#136783', '#1597A5', '#FEB705',
              '#F3A261', '#FA8600', '#E9C46B', '#F4A261']
    for proc_id, tasks in processor_schedules.items():
        for task_info in tasks:
            task = task_info['task']
            task_num = int(task[1:])
            color = colors[(task_num - 1) % len(colors)]
            ax.barh(y=proc_id, width=task_info['duration'], left=task_info['start'], height=0.6, color=color, edgecolor='black', linewidth=1)
            ax.text(task_info['start'] + task_info['duration']/2, proc_id, task, ha='center', va='center', fontsize=12, fontweight='bold')
    ax.set_yticks(range(num_processors))
    ax.set_yticklabels([f'P{i}' for i in range(num_processors)])
    ax.set_xlabel('Time', fontsize=14)
    ax.set_ylabel('Processor', fontsize=14)
    ax.set_title(title, fontsize=16)
    ax.axvline(x=makespan, color='red', linestyle='--', linewidth=2, label=f'Makespan = {makespan:.1f}')
    ax.legend(loc='upper right')
    ax.set_xlim(0, makespan * 1.1)
    ax.set_ylim(-0.5, num_processors - 0.5)
    ax.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()
    return fig

# --- 3. Rank Calculation ---
def upward_rank(graph, w_avg, comm_cost):
    rank = {}
    top = list(nx.topological_sort(graph))[::-1]
    for i in top:
        if not list(graph.successors(i)):
            rank[i] = w_avg[task_index[i]]
        else:
            rank[i] = w_avg[task_index[i]] + max(comm_cost[(i, j)] + rank[j] for j in graph.successors(i))
    return dict(rank)
w_avg = [sum(row)/len(row) for row in W]
rank_u = upward_rank(graph, w_avg, comm_cost)

# --- 4. HEFT Processor Allocation (EFT) ---
def eft_scheduler(graph, task_order, W, comm_cost, task_index):
    num_processors = len(W[0])
    processor_avail = [0] * num_processors
    task_schedule = {}
    processor_schedules = defaultdict(list)
    for task in task_order:
        task_idx = task_index[task]
        best_processor, best_eft, best_est = 0, float('inf'), 0
        for proc in range(num_processors):
            est = processor_avail[proc]
            for pred in graph.predecessors(task):
                if pred in task_schedule:
                    pred_finish_time = task_schedule[pred]['finish_time']
                    pred_processor = task_schedule[pred]['processor']
                    comm_delay = comm_cost.get((pred, task), 0) if pred_processor != proc else 0
                    est = max(est, pred_finish_time + comm_delay)
            execution_time = W[task_idx][proc]
            eft = est + execution_time
            if eft < best_eft:
                best_eft, best_processor, best_est = eft, proc, est
        task_schedule[task] = {
            'processor': best_processor,
            'start_time': best_est,
            'finish_time': best_eft,
            'execution_time': W[task_idx][best_processor]
        }
        processor_avail[best_processor] = best_eft
        processor_schedules[best_processor].append({
            'task': task,
            'start': best_est,
            'finish': best_eft,
            'duration': W[task_idx][best_processor]
        })
    makespan = max(task_schedule[task]['finish_time'] for task in task_schedule)
    return task_schedule, processor_schedules, makespan

def conv_visualizer(mean_abs_diffs, window_size=100):
  mean_abs_diffs_np = np.array(mean_abs_diffs)
  moving_avg = np.convolve(mean_abs_diffs_np, np.ones(window_size)/window_size, mode='valid')
  plt.figure(figsize=(10,5))
  plt.plot(mean_abs_diffs, color='grey', label='Raw')
  plt.plot(range(window_size-1, len(mean_abs_diffs)), moving_avg, color='blue', label='Moving Average')
  plt.xlabel('Episode')
  plt.ylabel('Mean Absolute Difference')
  plt.title('Mean Absolute Difference (Smoothed)')
  plt.legend()
  plt.show()

"""## 5. HEFT: Baseline (Ranku ordering, EFT allocation)


*   Base Algorithm


"""

ranku_ordering = list(dict(sorted(rank_u.items(), key=lambda item: item[1])).keys())[::-1]
task_schedule, processor_schedules, makespan = eft_scheduler(
    graph, ranku_ordering, W, comm_cost, task_index
)
print(f"Makespan: {makespan}")
for task, info in task_schedule.items():
    print(f"{task}: Processor {info['processor']}, Start: {info['start_time']:.1f}, Finish: {info['finish_time']:.1f}")
num_processors = len(W[0])
fig = visualize_gantt_chart(task_schedule, processor_schedules, makespan, num_processors, title="HEFT (Ranku+EFT)")

"""## 6. QL-HEFT: Large-State Q-learning (Q[state, action])



*   (Q, A) => (Set of all tasks that have been scheduled, Ready Task that can added)


"""

def episode_large_state(Q, epsilon=0.1, alpha=0.1, gamma=0.9):
    scheduled = frozenset()
    while len(scheduled) != len(graph.nodes):
        candidate_set = viable_tasks(scheduled)
        if random.random() < epsilon:
            y = random.choice(candidate_set)
        else:
            q_vals = {a: Q.get((scheduled, a), 0) for a in candidate_set}
            max_q = max(q_vals.values())
            best_actions = [a for a, v in q_vals.items() if v == max_q]
            y = random.choice(best_actions)
        reward = rank_u[y]
        next_state = scheduled.union({y})
        next_viable = viable_tasks(next_state)
        best = max(Q.get((next_state, a), 0) for a in next_viable) if next_viable else 0
        old = Q.get((scheduled, y), 0)
        Q[(scheduled, y)] = old + alpha * (reward + gamma * best - old)
        scheduled = next_state
    return Q

Q = defaultdict(float)
for i in range(10000):
    Q = episode_large_state(Q, epsilon=0.1)

optimal_task_order = []
state = frozenset()
while len(optimal_task_order)!=len(graph.nodes):
    legal = viable_tasks(state)
    paths = {a: Q.get((state, a), 0) for a in legal}
    best = max(paths, key=paths.get)
    optimal_task_order.append(best)
    state = state.union({best})
task_schedule, processor_schedules, makespan = eft_scheduler(
    graph, optimal_task_order, W, comm_cost, task_index
)
print(f"Makespan: {makespan}")
fig = visualize_gantt_chart(task_schedule, processor_schedules, makespan, num_processors, title="QL-HEFT (Large State)")

"""## 7. QL-HEFT: Small-State Q-learning (Q[last_task, next_task])"""

def episode_small_state(Q, epsilon=0.1, alpha=0.1, gamma=0.9):
    scheduled = set()
    entry_tasks = [t for t in graph.nodes if graph.in_degree(t) == 0 and t not in scheduled]
    last_task = random.choice(entry_tasks)
    scheduled.add(last_task)
    while len(scheduled) != len(graph.nodes):
        candidate_set = viable_tasks(scheduled)
        candidate_set = [t for t in candidate_set if t not in scheduled]
        if not candidate_set:
            break
        if random.random() < epsilon:
            y = random.choice(candidate_set)
        else:
            q_vals = {a: Q.get((last_task, a), 0) for a in candidate_set}
            max_q = max(q_vals.values())
            best_actions = [a for a, v in q_vals.items() if v == max_q]
            y = random.choice(best_actions)
        reward = rank_u[y]
        old = Q.get((last_task, y), 0)
        scheduled.add(y)
        next_candidates = viable_tasks(scheduled)
        next_candidates = [t for t in next_candidates if t not in scheduled]
        best = max(Q.get((y, a), 0) for a in next_candidates) if next_candidates else 0
        Q[(last_task, y)] = old + alpha * (reward + gamma * best - old)
        last_task = y
    return Q

def extract_task_order(Q):
    task_order = []
    scheduled = set()
    entry_tasks = [t for t in graph.nodes if graph.in_degree(t) == 0]
    last_task = max(entry_tasks, key=lambda t: Q.get((None, t), 0))
    task_order.append(last_task)
    scheduled.add(last_task)
    while len(scheduled) != len(graph.nodes):
        candidates = viable_tasks(scheduled)
        candidates = [t for t in candidates if t not in scheduled]
        if not candidates:
            break
        q_vals = {a: Q.get((last_task, a), 0) for a in candidates}
        max_q = max(q_vals.values())
        best_actions = [a for a, v in q_vals.items() if v == max_q]
        next_task = random.choice(best_actions)
        task_order.append(next_task)
        scheduled.add(next_task)
        last_task = next_task
    return task_order

Q = defaultdict(float)
for i in range(50000):
    Q = episode_small_state(Q, epsilon=0.1)
optimal_task_order = extract_task_order(Q)
task_schedule, processor_schedules, makespan = eft_scheduler(
    graph, optimal_task_order, W, comm_cost, task_index
)
print(f"Makespan: {makespan}")
fig = visualize_gantt_chart(task_schedule, processor_schedules, makespan, num_processors, title="QL-HEFT (Small State)")

"""## 7. QL-HEFT: Small-State Q-learning (Q[last_task, next_task]) fully functional"""

def episode_smallstate_epsilon(Q, epsilon=0.2, learning_rate=0.1, discount=0.9, ):
    scheduled = set()
    entry_tasks = [t for t in graph.nodes if graph.in_degree(t) == 0 and t not in scheduled]
    last_task = random.choice(entry_tasks)
    scheduled.add(last_task)
    abs_diffs = []
    while len(scheduled) != len(graph.nodes):
        candidate_set = viable_tasks(scheduled)
        candidate_set = [t for t in candidate_set if t not in scheduled]
        if not candidate_set:
            break
        if random.random() < epsilon:
            y = random.choice(candidate_set)
        else:
            q_vals = {a: Q.get((last_task, a), 0) for a in candidate_set}
            max_q = max(q_vals.values())
            best_actions = [a for a, v in q_vals.items() if v == max_q]
            y = random.choice(best_actions)
        reward = rank_u[y]
        old = Q.get((last_task, y), 0)
        scheduled.add(y)
        next_candidates = viable_tasks(scheduled)
        next_candidates = [t for t in next_candidates if t not in scheduled]
        best = max(Q.get((y, a), 0) for a in next_candidates) if next_candidates else 0
        Q[(last_task, y)] = old + learning_rate * (reward + discount * best - old)
        abs_diffs.append(abs(old - Q[(last_task, y)]))
        last_task = y
    return Q, sum(abs_diffs)/len(abs_diffs) if abs_diffs else 0


def show_q_table(Q, top_n=15):
    print(f"\nTop {top_n} Q-values:")
    for (last_task, action), value in sorted(Q.items(), key=lambda x: x[1], reverse=True)[:top_n]:
        print(f"LastTask={last_task}, Action={action}, Q={value:.3f}")

def extract_task_order(Q):
    task_order = []
    scheduled = set()
    entry_tasks = [t for t in graph.nodes if graph.in_degree(t) == 0]
    last_task = max(entry_tasks, key=lambda t: Q.get((None, t), 0))
    task_order.append(last_task)
    scheduled.add(last_task)
    while len(scheduled) != len(graph.nodes):
        candidates = viable_tasks(scheduled)
        candidates = [t for t in candidates if t not in scheduled]
        if not candidates:
            break
        q_vals = {a: Q.get((last_task, a), 0) for a in candidates}
        max_q = max(q_vals.values())
        best_actions = [a for a, v in q_vals.items() if v == max_q]
        next_task = random.choice(best_actions)
        task_order.append(next_task)
        scheduled.add(next_task)
        last_task = next_task
    return task_order

def full_run(epsilon, learning_rate, discount, window, threshold):
    Q = defaultdict(float)
    recent_diffs = []
    mean_abs_diffs = []
    converged = False
    episodes = 0
    while not converged and episodes < 300000:
        Q, mean_abs_diff = episode_smallstate_epsilon(
            Q, epsilon=epsilon, learning_rate=learning_rate, discount=discount
        )
        recent_diffs.append(mean_abs_diff)
        mean_abs_diffs.append(mean_abs_diff)
        if len(recent_diffs) > window:
            recent_diffs.pop(0)
        if len(recent_diffs) == window and np.mean(recent_diffs) < threshold:
            converged = True
            # print(f"Converged after {episodes} episodes")
        episodes += 1
    return Q, mean_abs_diffs, episodes

Q, mean_abs_diffs, episodes = full_run(epsilon=0.2, learning_rate=0.1, discount=0.8, window=40, threshold=0.2)
optimal_task_order = extract_task_order(Q)
task_schedule, processor_schedules, makespan = eft_scheduler(
    graph, optimal_task_order, W, comm_cost, task_index
)
print(f"Makespan: {makespan}")
num_processors = len(W[0])
fig = visualize_gantt_chart(task_schedule, processor_schedules, makespan, num_processors, title="QL-HEFT (Small State)")

optimal_task_order

def plot_q_table_matrix(Q, task_list):
    n = len(task_list)
    task_to_idx = {t: i for i, t in enumerate(task_list)}
    q_matrix = np.zeros((n, n))

    for i, t1 in enumerate(task_list):
        for j, t2 in enumerate(task_list):
            q_matrix[i, j] = Q.get((t1, t2), 0)

    df = pd.DataFrame(q_matrix, index=task_list, columns=task_list)
    """plt.figure(figsize=(n, n))
    plt.title("Q-table as n×n Matrix")
    ax = sns.heatmap(df, annot=True, fmt=".2f", cmap="viridis")
    plt.xlabel("Action (Next Task)")
    plt.ylabel("State (Last Task)")
    plt.show()"""
    return df


plot_q_table_matrix(Q, optimal_task_order)

conv_visualizer(mean_abs_diffs, window_size=100)

"""## 7. QL-HEFT: Small-State Q-learning (Q[last_task, next_task]) decay


"""

def episode_smallstate_epsilon(Q, epsilon=0.2, learning_rate=0.1, discount=0.9, episode_num=1, decay_type="harmonic"):
    # Compute decayed learning rate
    if decay_type == "harmonic":
        lr = learning_rate / (1 + episode_num)
    elif decay_type == "exponential":
        lr = learning_rate * (0.99 ** math.sqrt(episode_num))
    else:
        lr = learning_rate

    scheduled = set()
    entry_tasks = [t for t in graph.nodes if graph.in_degree(t) == 0 and t not in scheduled]
    last_task = random.choice(entry_tasks)
    scheduled.add(last_task)
    abs_diffs = []
    while len(scheduled) != len(graph.nodes):
        candidate_set = viable_tasks(scheduled)
        candidate_set = [t for t in candidate_set if t not in scheduled]
        if not candidate_set:
            break
        if random.random() < epsilon:
            y = random.choice(candidate_set)
        else:
            q_vals = {a: Q.get((last_task, a), 0) for a in candidate_set}
            max_q = max(q_vals.values())
            best_actions = [a for a, v in q_vals.items() if v == max_q]
            y = random.choice(best_actions)
        reward = rank_u[y]
        old = Q.get((last_task, y), 0)
        scheduled.add(y)
        next_candidates = viable_tasks(scheduled)
        next_candidates = [t for t in next_candidates if t not in scheduled]
        best = max(Q.get((y, a), 0) for a in next_candidates) if next_candidates else 0
        Q[(last_task, y)] = old + lr * (reward + discount * best - old)
        abs_diffs.append(abs(old - Q[(last_task, y)]))
        last_task = y
    return Q, sum(abs_diffs)/len(abs_diffs) if abs_diffs else 0

def extract_task_order(Q):
    task_order = []
    scheduled = set()
    entry_tasks = [t for t in graph.nodes if graph.in_degree(t) == 0]
    last_task = max(entry_tasks, key=lambda t: Q.get((None, t), 0))
    task_order.append(last_task)
    scheduled.add(last_task)
    while len(scheduled) != len(graph.nodes):
        candidates = viable_tasks(scheduled)
        candidates = [t for t in candidates if t not in scheduled]
        if not candidates:
            break
        q_vals = {a: Q.get((last_task, a), 0) for a in candidates}
        max_q = max(q_vals.values())
        best_actions = [a for a, v in q_vals.items() if v == max_q]
        next_task = random.choice(best_actions)
        task_order.append(next_task)
        scheduled.add(next_task)
        last_task = next_task
    return task_order

def full_run(epsilon, learning_rate, discount, window, threshold, decay_type="harmonic"):
    Q = defaultdict(float)
    recent_diffs = []
    mean_abs_diffs = []
    converged = False
    episodes = 0
    while not converged and episodes < 300000:
        Q, mean_abs_diff = episode_smallstate_epsilon(
            Q,
            epsilon=epsilon,
            learning_rate=learning_rate,
            discount=discount,
            episode_num=episodes,
            decay_type=decay_type
        )
        recent_diffs.append(mean_abs_diff)
        mean_abs_diffs.append(mean_abs_diff)
        if len(recent_diffs) > window:
            recent_diffs.pop(0)
        if len(recent_diffs) == window and np.mean(recent_diffs) < threshold:
            converged = True
        episodes += 1
    return Q, mean_abs_diffs, episodes

Q, mean_abs_diff, num_episodes = full_run(epsilon=0.3, learning_rate=0.05, discount=0.8, window=1000, threshold=0.0001, decay_type="exponential")
optimal_task_order = extract_task_order(Q)
task_schedule, processor_schedules, makespan = eft_scheduler(
    graph, optimal_task_order, W, comm_cost, task_index
)
print(f"Makespan: {makespan}")
num_processors = len(W[0])
fig = visualize_gantt_chart(task_schedule, processor_schedules, makespan, num_processors, title="QL-HEFT (Small State)")

def plot_q_table_matrix(Q, task_list):
    n = len(task_list)
    task_to_idx = {t: i for i, t in enumerate(task_list)}
    q_matrix = np.zeros((n, n))

    for i, t1 in enumerate(task_list):
        for j, t2 in enumerate(task_list):
            q_matrix[i, j] = Q.get((t1, t2), 0)

    df = pd.DataFrame(q_matrix, index=task_list, columns=task_list)
    """plt.figure(figsize=(n, n))
    plt.title("Q-table as n×n Matrix")
    ax = sns.heatmap(df, annot=True, fmt=".2f", cmap="viridis")
    plt.xlabel("Action (Next Task)")
    plt.ylabel("State (Last Task)")
    plt.show()"""
    return df


plot_q_table_matrix(Q, optimal_task_order)

conv_visualizer(mean_abs_diffs, window_size=100)

